{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Learning Goals (EDIT)\n",
    "In this lab, we'll explore different models used to predict which of several labels applies to a new datapoint based on labels observed in the training data.\n",
    "\n",
    "By the end of this lab, you should:\n",
    "- Be familiar with the `sklearn` implementations of\n",
    " - Linear Regression\n",
    " - Logistic Regression\n",
    "- Be able to make an informed choice of model based on the data at hand\n",
    "- (Bonus) Structure your sklearn code into Pipelines to make building, fitting, and tracking your models easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# IMPORTS GALORE\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1:  The AirBnB NYC 2019 Dataset + EDA\n",
    "The dataset contains information about AirBnB hosts in NYC from 2019. There are 49k unique hosts and 16 features for each:\n",
    "\n",
    "- **id:** listing ID\n",
    "- **name:** name of the listing\n",
    "- **host_id:** host ID\n",
    "- **host_name:** name of the host\n",
    "- **neighbourhood_group:** NYC borough\n",
    "- **neighbourhood:** neighborhood\n",
    "- **latitude:** latitude coordinates\n",
    "- **longitude:** longitude coordinates\n",
    "- **room_type:** listing space type (e.g., private room, entire home)\n",
    "- **price:** price in dollars per night\n",
    "- **minimum_nights:** number of min. nights required for booking\n",
    "- **number_of_reviews:** number of reviews\n",
    "- **last_review:** date of the last review\n",
    "- **reviews_per_month:** number of reviews per month\n",
    "- **calculated_host_listings_count:** number of listings the host has\n",
    "- **availability_365:** number of days the listing is available for booking\n",
    "\n",
    "Our goal is to predict the price of unseen housing units as being 'affordable' or 'unaffordable', by using their features. We will assume that this task is for a particular client who has a specific budget and would like to simplify the problem by classifying any unit that costs \\< \\\\$150 per night as 'affordable' and any unit that costs \\\\$150 or great as 'unaffordable'.\n",
    "\n",
    "For this task, we will exercise our normal data science pipeline -- from EDA to modelling and visualization. In particular, we will show the performance of 2 classifiers:\n",
    "\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "\n",
    "Let's get started! And awaaaaay we go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read-in and checking\n",
    "We do the usual read-in and verification of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Answer\n",
    "#open as dataframe nyc_aribnb.csv\n",
    "#...\n",
    "#Look at the first rows\n",
    "#...d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the training/dev/testing data\n",
    "As usual, we split the data before we begin our analysis. It would be unfair to cheat by looking at the testing data. Let's divide the data into 60% training, 20% development (aka validation), 20% testing. However, before we split the data, let's make the simple transformation and converting the prices into a categories of being affordable or not.\n",
    "If it is smaller than 150 it affordable and has a value of 1 whenever and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer\n",
    "df['affordable'] =\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The `affordable` column now has a value of 1 whenever the price is < 150, and 0 otherwise.\n",
    "\n",
    "Also, the feature named `neighbourhood_group` can be easily confused with `neighbourhood`, so let's go ahead and rename it to `borough`, as that is more distinct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer\n",
    "df.rename(, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we can look better at the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"exercise\"><b>Exercise :</b> What do you think about the values? What will you do?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split the data while ensuring that our test set has a fair distribution of affordable units, then further split our training set so as to create the development set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['affordable'])\n",
    "df_train, df_dev = train_test_split(df_train, test_size=0.25, random_state=99) #stratify=df_train['affordable'])\n",
    "\n",
    "# ensure our dataset splits are of the % sizes we want\n",
    "total_size = len(df_train) + len(df_dev) + len(df_test)\n",
    "print(\"train:\", len(df_train), \"=>\", len(df_train) / total_size)\n",
    "print(\"dev:\", len(df_dev), \" =>\", len(df_dev) / total_size)\n",
    "print(\"test:\", len(df_test), \"=>\", len(df_test) / total_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the target value (i.e., __affordable__) from our current dataframes and create it as separate prediction dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "x_train = df_train.drop(['price', 'affordable'], axis=1)\n",
    "y_train = pd.DataFrame(data=df_train['affordable'], columns=[\"affordable\"])\n",
    "\n",
    "# dev\n",
    "x_dev = df_dev.drop(['price', 'affordable'], axis=1)\n",
    "y_dev = pd.DataFrame(data=df_dev['affordable'], columns=[\"affordable\"])\n",
    "\n",
    "# test\n",
    "x_test = df_test.drop(['price', 'affordable'], axis=1)\n",
    "y_test = pd.DataFrame(data=df_test['affordable'], columns=[\"affordable\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now onwards, we will do EDA and cleaning based on the training set, `x_train`. Check NaN values in all the columns and draw conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the summary statistics of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about the values? Do you think something wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.hist(x_train['XXXXXXXXXXXX'], 25, log=True)\n",
    "plt.xlabel('minimum_nights')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yea, that instance was a strong outlier, and the host was being ridiculously greedy. That's a clever way to get out a multi-year lease. Notice that we are using log-scale. Clearly, a lot of our mass is from units less than 365 days. To get a better sense of that subset, let's re-plot only units with minumum_nights < 365 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = x_train['minimum_nights']<365\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.hist(x_train['minimum_nights'][subset], 30, log=True)\n",
    "plt.xlabel('minimum_nights')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that doesn't look too bad, as most units require < 30 nights. It's surprising that some hosts list an unreasonable requirement for the minimum number of nights. There is a risk that any host that lists such an unreasonable value might also have other incorrect information. Personally, I think anything beyond 30 days could be suspicious. If we were to exclude any unit that requires more than 30 days, how many instances would we be ignoring?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train.loc[x_train['minimum_nights']>30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we'd be throwing away 436 out of our ~30k entries. That's roughly 1.5\\% of our data. While we generally want to keep and use as much data as we can, I think this is an okay amount to discard, especially considering (1) we have a decently large amount of data remaining, and (2) the entries beyond a 30-day-min could be unrealiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_subset = x_train['minimum_nights'] <= 30\n",
    "x_train = x_train.loc[good_subset]\n",
    "y_train = y_train.loc[good_subset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we only trimmed our training data, not our development or testing data. I am making this choice because in real scenarios, we would not know the nature of the testing data values. We pre-processed our data to ignore all data that has a price of $0, and to ignore certain columns (even if it's in the testing set), but that was fair because those columns proved to be obvious, bogus element of the dataset. However, it would be unfair to inspect the values of the training set and then to further trim the development and testing set accordingly, conditioned on certain data values.\n",
    "\n",
    "The remaining columns of our training data all have reasonable summary statistics. None of the min's or max's are cause for concern, and we have no reason to assert a certain distribution of values. Since all the feature values are within reasonable ranges, and there are no missing values (NaNs) remaining, we can confidently move foward. To recap, our remaining columns are now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in x_train.columns] # easier to read vertically than horizontally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have a terribly large number of features. This allows us to inspect every pairwise interaction. A scatterplot is great for this, as it provides us with a high-level picture of how every pair of features correlates. If any subplot of features depicts a linear relationship (i.e., a clear, concise path with mass concentrated together), then we can assume there exists some collinearity -- that the two features overlap in what they are capturing and that they are not independent from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(x_train, figsize=(30,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Predicting with Linear Regression\n",
    "\n",
    "Now, let's actually use our features to make more informed predictions. Since our model needs to use numeric values, not textual ones, let's use **ONLY** the following features for our linear model:\n",
    "\n",
    "- `borough`, using 1-hot encodings. There are 5 distinct boroughs, so represent them via 4 unique columns.\n",
    "- `latitude`\n",
    "- `longitude`\n",
    "- `room_type`, using 1-hot encodings. There are 3 distinct room_types, so represent them via 2 unique columns.\n",
    "- `minimum_nights`\n",
    "- `number_of_reviews`\n",
    "- `calculated_host_listings_count`\n",
    "- `availability_365`\n",
    "\n",
    "<br>\n",
    "<div class=\"exercise\"><b>Exercise 2:</b> Convert `x_train` to have only the columns listed above. The shape should be 28,894 x 12 </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [SOLUTION:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 3:</b> For this exercise, perform multi-linear regression and evaluate it on the development set. Do not introduce any polynomial terms or any other new features. Any prediction that is >= 0.5 should be treated as being an 'affordable' prediction. Anything below 0.5 should be 'unaffordable'. What is your accuracy %? (). Is this what you expected? Is this reasonable, and if not, what do you think are the issues?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [SOLUTION HERE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=OLS(y_train_lr, x_train_padded)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 3b:</b> Does the code run? if not why? what should you change?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "y_hat_dev = results.predict(exog=x_dev_padded)\n",
    "\n",
    "# calculating and reporting the requested values, particularly the Test R^2\n",
    "print('Train R^2 = {:.4}'.format(results.rsquared))\n",
    "print('Test R^2 = {:.4}'.format(r2_score(y_dev_lr, y_hat_dev)))\n",
    "\n",
    "# i'm using numpy's round() function, instead of manually checking for values above 0.5\n",
    "accuracy_score(y_dev, [np.round(np.array(y_hat_dev)[ind]) for ind in range(np.size(np.array(y_hat_dev)))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 4:</b> Regularize your model via Ridge regression and Lasso regression. Specifically, report the model's accuracy on the development set; do so while varying the alpha (aka lambda) parameter to be each of these values: [.001, .01, .05, .1, .5, 1, 5, 10, 50, 100, 500]). What is your best result?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [SOLUTION HERE]\n",
    "best_accuracy = -1\n",
    "best_model = None\n",
    "for cur_alpha in [0.001, .01, .05, .1, .5, 1, 5, 10, 50, 100, 500]:\n",
    "\n",
    "    # fit (using Ridge Regression), predict, and score\n",
    "    fitted_ridge = Ridge(alpha=cur_alpha).fit(x_train, y_train_lr)\n",
    "    y_hat_dev = fitted_ridge.predict(x_dev).reshape(1,-1)[0]\n",
    "    \n",
    "    cur_accuracy = accuracy_score(y_dev['affordable'].to_numpy(), np.round(y_hat_dev))\n",
    "    if cur_accuracy > best_accuracy:\n",
    "        best_accuracy = cur_accuracy\n",
    "        best_model = fitted_ridge\n",
    "    \n",
    "    # fit (using Lasso Regression), predict, and score\n",
    "    fitted_lasso = Lasso(alpha=cur_alpha).fit(x_train, y_train_lr)\n",
    "    y_hat_dev = fitted_lasso.predict(x_dev).reshape(1,-1)[0]\n",
    "    cur_accuracy = accuracy_score(y_dev['affordable'].to_numpy(), np.round(y_hat_dev))\n",
    "    if cur_accuracy > best_accuracy:\n",
    "        best_accuracy = cur_accuracy\n",
    "        best_model = fitted_lasso\n",
    "    \n",
    "print(\"best_model:\", best_model, \"yielded accuracy of:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note,** we did not perform cross-validation, so perhaps our model could have performed even better, had we done so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 5:</b> Plot two histograms of the residuals from your best performing linear regression model (having trained on the training set, one plot should show the distribution of training set residuals and another plot for the distribution of development set residuals). Does this adhere to the assumptions of a linear model?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [SOLUTION HERE]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 4: Binary Logistic Regression\n",
    "Linear regression is usually a good baseline model, but since the outcome we're trying to predict only takes values 0 and 1 we'll want to use logistic regression instead of basic linear regression.\n",
    "\n",
    "We will use `sklearn` for now, but `statsmodels` also provides LogisticRegression, along with nifty features like confidence intervals.\n",
    "\n",
    "First, let's import the necessary classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#answer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, let's instantiate a new LogisticRegression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can fit our model with just 1 line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x_train, y_train['affordable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 6:</b> Using .predict(), make predictions on the development set </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [.predict()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) documentation here. **NOTE:** regularization is applied by default. Especially pay attention to the following arguments/parameters:\n",
    "\n",
    "- **C** penalty, which we discussed in class. Experiment with varying values from 0 to 100 million! \n",
    "- **max_iterations**: experiment with values from 5 to 5000. Do you expect more iterations to always perform better? Why or why not?\n",
    "- **penalty**: for designating L1 (Lasso) or L2 (Ridge) loss; default is L2\n",
    "- **solver**: especially for the multi-class setting\n",
    "\n",
    "After fitting the model, you can print the ``.coef_`` value to see its coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_dev = lr.predict(x_dev)\n",
    "initial_score = accuracy_score(y_dev['affordable'].to_numpy(), y_hat_dev)\n",
    "print(\"our initial logistic regression model yielded accuracy score of:\", initial_score)\n",
    "\n",
    "best_accuracy = -1\n",
    "best_model = None\n",
    "\n",
    "# experiment with different values\n",
    "c_vals = [1, 10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "num_iters = [5, 10, 100, 1000, 5000]\n",
    "for c_val in c_vals:\n",
    "    for num_iter in num_iters:\n",
    "\n",
    "        if cur_accuracy > best_accuracy:\n",
    "\n",
    "\n",
    "print(\"best logistic regression model:\", lr, \"yielded an accuracy score:\", best_accuracy)\n",
    "print(\"its learned coefficients:\", len(best_model.coef_[0]))\n",
    "print(\"the coefficients align with our features:\", x_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here should show that for this dataset, logistic regression offered effectively identical performance as linear regression. There are two main takeaways from this:\n",
    "- logistic regression should not be viewed as being _superior_ to linear regression; it should be viewed as a solution to a different type of problem -- **classification** (predicting categorical outputs), not **regression** (predicting continuous-valued outputs).\n",
    "- In our situation, our two categories/classes (affordable or not) had an ordinal nature. That is, the continuum of prices directly aligned with the structure of our two classes. Alternatively, you could imagine other scenarios where our two categories are nominal and thus un-rankable (e.g., predicting cancer or not, or predicting which NYC borough an AirBnB is in based on its property features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 5 (The Real Challenge): Multiclass Classification\n",
    "Before we move on, let's consider a more common use case of logistic regression: predicting not just a binary variable, but what level a categorical variable will take. Instead of breaking the price variable into two classes (affordable being true or false), we may care for more fine-level granularity.\n",
    "\n",
    "For this exercise, go back to the original `df` dataframe and construct 5 classes of pricing:\n",
    "\n",
    "- budget: < 80\n",
    "- affordable: 80 < x < 120\n",
    "- average: 120 < x < 180\n",
    "- expensive: 180 < x < 240\n",
    "- very expensive: 240 < x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `cut` function obviously stores a lot of extra information for us. It's a very useful tool for discretizing an existing variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<div class=\"exercise\"><b>Exercise 8:</b> After making the new categories, perform the same predictions as above. Compare your results. What improvements could we make? (not just w/ the parameters, but with possibly expanding and using other features from our original dataset!)</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates multi-class labels for training\n",
    "x_train_multiclass = x_train.copy()\n",
    "x_train_multiclass['price_level'] = pd.cut(df_train['price'],[0,80,120,180,240,float('inf')], labels=[0,1,2,3,4])\n",
    "y_train_multiclass = pd.DataFrame(data=x_train_multiclass['price_level'], columns=[\"price_level\"])\n",
    "x_train_multiclass = x_train_multiclass.drop(['price_level'], axis=1)\n",
    "\n",
    "# creats multi-class labels for dev\n",
    "x_dev_multiclass = x_dev.copy()\n",
    "x_dev_multiclass['price_level'] = pd.cut(df_dev['price'],[0,80,120,180,240,float('inf')], labels=[0,1,2,3,4])\n",
    "y_dev_multiclass = pd.DataFrame(data=x_dev_multiclass['price_level'], columns=[\"price_level\"])\n",
    "x_dev_multiclass = x_dev_multiclass.drop(['price_level'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = -1\n",
    "best_model = None\n",
    "\n",
    "# experiment with different values\n",
    "c_vals = [1, 10, 100, 1000, 10000]\n",
    "num_iters = [10, 100, 1000, 5000]\n",
    "for c_val in c_vals:\n",
    "    for num_iter in num_iters:\n",
    "\n",
    "        print(cur_accuracy)\n",
    "        if cur_accuracy > best_accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best logistic regression model:\", lr, \"yielded an accuracy score:\", best_accuracy)\n",
    "print(\"its learned coefficients:\", len(best_model.coef_[0]))\n",
    "print(\"the coefficients align with our features:\", x_dev.shape)\n",
    "for i in range(len(x_dev.columns)):\n",
    "    print(\"feature:\", x_dev.columns[i], \"; coef:\", best_model.coef_[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
